{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injesting the worksheets from FUZZYWUZZY matching\n",
    "After several iterations for FUZZYWUZZY matching, I used EXCEL in .xlsx format and multiple tabs to go through and manually match everthing not found with 90% Fuzzy matching.\n",
    "\n",
    "Using NUMPY (to work with Excel .xlsx file) I imported each tab as a dataframe and inserted them into the database (append)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connect to the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbconn = sqlite3.connect(':memory:')\n",
    "dbconn = sqlite3.connect('pars.db')\n",
    "print(dbconn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor = dbconn.cursor()\n",
    "cursor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the IN files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = r\"C:\\Users\\CHUGHES\\Documents\\PROJECTS\\Python\\001-PARS\\PRODUCT\\2014-2021-INQUIRY-WORKSHEET.xlsx\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlfile = pd.ExcelFile(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlfile.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I had created a tab with each year of PARs.  Here I named them one at a time and created a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=xlfile.parse('2021') #  Read the sheet to a dataframe\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = map(str.upper, df.columns) #  Make all the column headers upper case\n",
    "# df['LNAME'] = df['LNAME'].str.upper()\n",
    "# df['FNAME'] = df['FNAME'].str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Need to change data type to a string for the Lamda function to work\n",
    "###  Also converting the string value of the Date fields to DateTime and \n",
    "### then to just a DATE\n",
    "\n",
    "Again, here I get all in the format I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype({\"LNAME\": str}, errors='raise') \n",
    "df = df.astype({\"FNAME\": str}, errors='raise')\n",
    "# df['OPEN_DATE'].dt.date\n",
    "df['OPEN_DATE'] = pd.to_datetime(df['OPEN_DATE']).dt.strftime('%Y-%m-%d')\n",
    "df['OPEN_DATE'] = pd.to_datetime(df['OPEN_DATE'])\n",
    "df['OPEN_DATE'] = pd.to_datetime(df['OPEN_DATE']).dt.date\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['CLOSED_DATE'] = pd.to_datetime(df['CLOSED_DATE']).dt.strftime('%Y-%m-%d')\n",
    "df['CLOSED_DATE'] = pd.to_datetime(df['CLOSED_DATE'])\n",
    "df['CLOSED_DATE'] = pd.to_datetime(df['CLOSED_DATE']).dt.date\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I append each dataframe into a table in SQLITE.  Then I use SQL for final clean and export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql('INQ',con = dbconn, if_exists='append', index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3ee07828cb95c6b5b1c8f5e8792024b55c7b92ed3d39e640de503f3abd63df5f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
